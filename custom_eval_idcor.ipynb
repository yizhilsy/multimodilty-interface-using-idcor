{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 载入训练好的llava架构的模型，在数据集`liuhaotian/LLaVA-CC3M-Pretrain-595K`上提取image_embeds和text_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import (\n",
    "    LlavaForConditionalGeneration,\n",
    "    LlavaProcessor,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    AutoProcessor\n",
    ")\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "from PIL import Image\n",
    "from peft import PeftModel\n",
    "\n",
    "import logging\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, List, Optional, Sequence\n",
    "\n",
    "import evaluate\n",
    "\n",
    "# custom package\n",
    "\n",
    "# 导入读取数据集和处理数据集为向量的工具类\n",
    "from show_llava.data import LlavaDataset, TrainLLavaModelCollator\n",
    "from show_llava.util import print_trainable_parameters\n",
    "# 导入计算id, idcor的package\n",
    "from utils import (\n",
    "    intrinsic_dimension, metrics, utils\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 指定gpu\n",
    "device = \"cuda:0\"\n",
    "# 初始化日志\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 加载模型及processor所需的工具类及工具函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 指定要训练的模型路径及训练参数工具类\n",
    "@dataclass\n",
    "class ModelArguments:\n",
    "    # 基础模型路径（ lora 微调基于的初始模型）\n",
    "    model_name_or_path: Optional[str] = field(default=\"./qwen2.5_3B_Instruct_clipvL14_model/model001\")\n",
    "    # lora 微调结果路径\n",
    "    lora_name_or_path: Optional[str] = field(default=\"./output_model_lora_show/[epoch4-5]qwen2.5_3B_Instruct_clipvL14\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_processor(modelargs: ModelArguments):\n",
    "    # 读取模型\n",
    "    model = LlavaForConditionalGeneration.from_pretrained(\n",
    "        modelargs.model_name_or_path,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        low_cpu_mem_usage=True,\n",
    "        device_map=device\n",
    "    )\n",
    "    # 合并lora微调模型\n",
    "    model = PeftModel.from_pretrained(model, modelargs.lora_name_or_path)\n",
    "    # 加载processor处理器\n",
    "    processor = LlavaProcessor.from_pretrained(modelargs.model_name_or_path)\n",
    "    return model, processor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 加载数据集所需的工具类和工具函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 指定数据集路径工具类\n",
    "@dataclass\n",
    "class DataArguments:\n",
    "    data_path: str = field(\n",
    "        default=None, metadata={\"help\": \"Path to the training data.\"}\n",
    "    )\n",
    "    # source_length: int = field(default=128)\n",
    "    # target_length: int = field(default=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset_collator(processor, dataargs: DataArguments):\n",
    "\n",
    "    llava_dataset = LlavaDataset(\n",
    "        dataargs.data_path  # \"data/liuhaotian/LLaVA-CC3M-Pretrain-595K\"\n",
    "    )\n",
    "    data_collator = TrainLLavaModelCollator(processor, -100)\n",
    "\n",
    "    return llava_dataset, data_collator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 设置`model_args`和`data_args`参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(ModelArguments(model_name_or_path='./qwen2.5_3B_Instruct_clipvL14_model/model001', lora_name_or_path='./output_model_lora_show/[epoch4-5]qwen2.5_3B_Instruct_clipvL14'),\n",
       " DataArguments(data_path='/home/lsy/shared_data/liuhaotian/LLaVA-CC3M-Pretrain-595K'))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_args: ModelArguments = ModelArguments(\n",
    "    model_name_or_path=\"./qwen2.5_3B_Instruct_clipvL14_model/model001\",\n",
    "    lora_name_or_path=\"./output_model_lora_show/[epoch4-5]qwen2.5_3B_Instruct_clipvL14\"\n",
    ")\n",
    "\n",
    "data_args: DataArguments = DataArguments(\n",
    "    data_path=\"/home/lsy/shared_data/liuhaotian/LLaVA-CC3M-Pretrain-595K\"\n",
    ")\n",
    "\n",
    "model_args, data_args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 载入要评测的模型和对应的`processor`，使用`lora`加载的方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e5402c7a65e49969d208743282ab791",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model, processor = load_model_processor(model_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 设置并加载读取数据相关的工具变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset, data_collator = load_dataset_collator(processor, data_args)\n",
    "dataloader_params = {\n",
    "    \"batch_size\": 8,\n",
    "    \"collate_fn\": data_collator,\n",
    "    \"num_workers\": 4,\n",
    "    \"pin_memory\": True,\n",
    "    \"persistent_workers\": 4,\n",
    "    \"shuffle\": False\n",
    "}\n",
    "eval_dataloader = DataLoader(eval_dataset, **dataloader_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### eval循环主体"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    all_text_embeds = []\n",
    "    all_image_embeds = []\n",
    "    for steps, inputs in tqdm(enumerate(eval_dataloader)):\n",
    "        print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### evaluate函数主体及创建Trainer（==转为python脚本的封装==）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "     # 将命令行参数解析成 dataclass 对象\n",
    "    parser = transformers.HfArgumentParser(\n",
    "        (ModelArguments, DataArguments, TrainingArguments)\n",
    "    )\n",
    "    model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n",
    "    model, processor = load_model_processor(model_args)\n",
    "    eval_dataset, data_collator = load_dataset_collator(processor, data_args)\n",
    "    \n",
    "\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llava",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
