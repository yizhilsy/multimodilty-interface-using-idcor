{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import pandas as pd\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from transformers import AutoProcessor\n",
    "from torch import Tensor\n",
    "from dataclasses import dataclass\n",
    "from .constants import IGNORE_INDEX, DEFAULT_IMAGE_TOKEN\n",
    "\n",
    "class finetune_SupervisedDataset(Dataset):\n",
    "    \"\"\" Dataset for supervised fine-tuning in Llava stage2 train \"\"\"\n",
    "    \n",
    "    def __init__(self, data_path: str, image_folder: str) -> None:\n",
    "        super().__init__()\n",
    "        self.data_path = data_path\n",
    "        self.image_folder = Path(image_folder)\n",
    "        self.chat_data = self.build_dataset(self.data_path)\n",
    "\n",
    "    def build_dataset(self, data_path: str) -> List[Dict[str, Any]]:\n",
    "        data_path = Path(data_path)\n",
    "        chat_data = pd.read_json(path_or_buf=data_path).to_dict(orient=\"records\")\n",
    "        return chat_data\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.chat_data)\n",
    "    \n",
    "    def __getitem__(self, index) -> tuple[str, str, Path]:\n",
    "        cur_data = self.chat_data[index]\n",
    "        human_input = cur_data['conversations'][0]['value']\n",
    "        gpt_output = cur_data['conversations'][1]['value']\n",
    "        image_path = self.image_folder.joinpath(cur_data.get('image'))\n",
    "        return (human_input, gpt_output, image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/home/lsy/shared_data/liuhaotian/LLaVA-Finetune/ScienceQA/llava_train_QCM-LEA.json\"\n",
    "image_folder = \"/home/lsy/shared_data/liuhaotian/LLaVA-Finetune/ScienceQA/images/train\"\n",
    "sqa_dataset = finetune_SupervisedDataset(data_path, image_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sqa_dataset), sqa_dataset[0]\n",
    "Image.open(sqa_dataset[0][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "@dataclass\n",
    "class QaImageOutput:\n",
    "    q_input_ids: torch.Tensor\n",
    "    pixel_values: torch.Tensor\n",
    "    a_input_ids: torch.Tensor\n",
    "\n",
    "def preprocess_multimodal(q_text: str):\n",
    "    if DEFAULT_IMAGE_TOKEN in q_text:\n",
    "        q_text = q_text.replace(DEFAULT_IMAGE_TOKEN, '').strip()\n",
    "        q_text = DEFAULT_IMAGE_TOKEN + '\\n' + q_text\n",
    "        q_text = q_text.strip()\n",
    "    return q_text\n",
    "\n",
    "def build_qaimage(processor: AutoProcessor, q_text: str, a_text: str, image_path: Path) -> QaImageOutput:\n",
    "    # adjust <image> position for instruction or input or question\n",
    "    q_text = preprocess_multimodal(q_text)\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": q_text},\n",
    "    ]\n",
    "    # 应用模板后将会应用speaker角色及start/end signal\n",
    "    prompt = processor.tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    # image\n",
    "    raw_image = Image.open(fp=image_path)\n",
    "    # 生成Question部分的向量\n",
    "    inputs = processor(text=prompt, images=raw_image, return_tensors=\"pt\")\n",
    "    # 生成Answer部分的向量\n",
    "    a_input_ids = processor.tokenizer(\n",
    "        a_text,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=\"longest\",\n",
    "        truncation=True,\n",
    "    )[\"input_ids\"]\n",
    "    return QaImageOutput(\n",
    "        q_input_ids=inputs[\"input_ids\"],\n",
    "        pixel_values=inputs[\"pixel_values\"],\n",
    "        a_input_ids=a_input_ids,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "# 定义 collator 函数\n",
    "class TrainLLavaModelCollator:\n",
    "    def __init__(self, processor: AutoProcessor, MY_IGNORE_INDEX: int) -> None:\n",
    "        self.processor = processor\n",
    "        self.ignore_index = MY_IGNORE_INDEX if MY_IGNORE_INDEX is not None else IGNORE_INDEX\n",
    "    \n",
    "    # 拼接单个样本的 q_input_ids 及 a_input_ids\n",
    "    def convert_one_piece(self,\n",
    "                          q_input_ids: torch.Tensor,\n",
    "                          a_input_ids: torch.Tensor) -> None:\n",
    "        input_ids = torch.concat(tensors=[\n",
    "            q_input_ids,\n",
    "            a_input_ids,\n",
    "            torch.tensor(data=self.processor.tokenizer.eos_token_id).reshape(1, -1)\n",
    "        ], axis=1)\n",
    "        labels = torch.concat([\n",
    "            torch.full_like(input=q_input_ids, fill_value=self.ignore_index),\n",
    "            a_input_ids,\n",
    "            torch.tensor(data=self.processor.tokenizer.eos_token_id).reshape(1, -1)\n",
    "        ], axis=1)\n",
    "        return input_ids, labels\n",
    "    \n",
    "    def __call__(self, features:List) -> tuple[Tensor, Tensor, Tensor, Tensor]:\n",
    "        input_ids_list = []\n",
    "        labels_list = []\n",
    "        pixel_values = []\n",
    "        max_input_len_list = []\n",
    "\n",
    "        for feature in features:\n",
    "            # 1. 调用 build_qaimage 函数将单个样本转换为张量\n",
    "            qaimage_output = build_qaimage(\n",
    "                processor=self.processor,\n",
    "                q_text=feature[0],\n",
    "                a_text=feature[1],\n",
    "                image_path=feature[2]\n",
    "            )\n",
    "            # 2. 将单个样本的 q_input_ids 及 a_input_ids 张量拼接\n",
    "            temp_input_ids, temp_labels = self.convert_one_piece(\n",
    "                q_input_ids=qaimage_output.q_input_ids,\n",
    "                a_input_ids=qaimage_output.a_input_ids\n",
    "            )\n",
    "            input_ids_list.append(temp_input_ids)\n",
    "            labels_list.append(temp_labels)\n",
    "            pixel_values.append(qaimage_output.pixel_values)\n",
    "            max_input_len_list.append(temp_input_ids.shape[1])\n",
    "        \n",
    "        # 对齐 input_ids 和 labels\n",
    "        max_input_len = max(max_input_len_list)\n",
    "        final_input_ids = torch.concat([    # 将所有对齐到最大长度后的 input_ids 拼接起来组成 final_input_ids\n",
    "            torch.concat([  # 遍历每个 input_ids 将它们对齐到最大长度\n",
    "                torch.full(size=(1, max_input_len - max_input_len_list[index]), fill_value=self.processor.tokenizer.pad_token_id),\n",
    "                value\n",
    "            ], axis=1)\n",
    "            for index, value in enumerate(iterable=input_ids_list)\n",
    "        ])\n",
    "\n",
    "        final_labels = torch.concat([    # 将所有对齐到最大长度后的 labels 拼接起来组成 final_labels\n",
    "            torch.concat([  # 遍历每个 labels 将它们对齐到最大长度\n",
    "                torch.full(size=(1, max_input_len - max_input_len_list[index]), fill_value=self.ignore_index),\n",
    "                value\n",
    "            ], axis=1)\n",
    "            for index, value in enumerate(iterable=labels_list)\n",
    "        ])\n",
    "\n",
    "        # 按照 dim=0 维拼接所有的 pixel_values\n",
    "        final_pixel_values = torch.concat(pixel_values, axis=0)\n",
    "        attention_mask = torch.ones_like(final_input_ids)\n",
    "        # 因对齐而造成的填充部分 attention_mask 置 0\n",
    "        attention_mask[final_input_ids == self.processor.tokenizer.pad_token_id] = 0\n",
    "        return {\n",
    "            \"input_ids\": final_input_ids,\n",
    "            \"labels\": final_labels,\n",
    "            \"pixel_values\": final_pixel_values,\n",
    "            \"attention_mask\": attention_mask\n",
    "        }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llava",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
