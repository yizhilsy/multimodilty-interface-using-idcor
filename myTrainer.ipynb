{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 自定义Trainer类，将idcor的计算也考虑到损失函数中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.5019,  0.9098, -0.3257,  ...,  1.2817,  0.0495, -1.5661],\n",
       "          [-0.2381,  0.6739,  0.4183,  ..., -1.1270, -0.1086, -0.1199],\n",
       "          [ 0.1480,  1.4203, -0.2630,  ...,  0.1286,  0.7086,  0.0828]],\n",
       " \n",
       "         [[ 0.9037, -0.6607,  0.5651,  ..., -2.1822,  0.1593,  0.7980],\n",
       "          [-1.1816, -0.9557,  1.1454,  ...,  0.7013,  0.0301,  1.1178],\n",
       "          [ 1.6244,  0.2782, -0.4068,  ...,  0.7782,  0.1591, -0.9041]],\n",
       " \n",
       "         [[ 2.9414,  0.9978, -0.6884,  ..., -0.7702,  1.2302, -0.4109],\n",
       "          [-1.3048,  0.6791, -1.3369,  ...,  0.5799, -0.1845, -0.2855],\n",
       "          [ 0.3539, -1.7124, -2.0652,  ...,  1.4186,  0.2590, -0.7188]],\n",
       " \n",
       "         [[ 1.3619, -0.9040, -0.6858,  ..., -0.4627,  0.1842,  0.3898],\n",
       "          [ 1.3845, -0.1301, -0.2596,  ..., -0.2234,  0.0809,  0.0944],\n",
       "          [-0.2858, -2.5773, -1.1297,  ..., -0.4675, -2.7245,  0.7337]]]),\n",
       " torch.Size([4, 3, 2048]))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "image_features = torch.randn(4, 3, 2048)\n",
    "image_features, image_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 2048])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_features_average = torch.mean(image_features, dim=1)\n",
    "image_features_average.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 2048])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.1973,  1.0013, -0.0568,  ...,  0.0944,  0.2165, -0.5344]]),\n",
       " torch.Size([1, 2048]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single_average_feature = image_features_average[0].unsqueeze(0)\n",
    "single_average_feature, single_average_feature.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import logging\n",
    "import os\n",
    "from dataclasses import dataclass, field\n",
    "from functools import partial\n",
    "from typing import Dict, List, Optional, Sequence\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    LlavaForConditionalGeneration,\n",
    "    LlavaProcessor,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "\n",
    "# 导入读取数据集和处理数据集为向量的工具类\n",
    "from show_llava.data import LlavaDataset, TrainLLavaModelCollator\n",
    "from show_llava.util import print_trainable_parameters\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 指定数据集路径工具类\n",
    "@dataclass\n",
    "class DataArguments:\n",
    "    data_path: str = field(\n",
    "        default=None, metadata={\"help\": \"Path to the training data.\"}\n",
    "    )\n",
    "    # source_length: int = field(default=128)\n",
    "    # target_length: int = field(default=512)\n",
    "\n",
    "# 指定要训练的模型路径及训练参数工具类\n",
    "@dataclass\n",
    "class ModelArguments:\n",
    "    model_name_or_path: Optional[str] = field(default=\"./show_model/model001\")\n",
    "    train_type: Optional[str] = field(\n",
    "        default=\"none\",\n",
    "        metadata={\n",
    "            \"help\": \"\"\"\n",
    "            1. use_lora:使用lora训练,\n",
    "            2. none:全量参数训练;\n",
    "            3. freeze_vision:只冻结vision_tower进行训练\n",
    "            4. freeze_vision_and_language:冻结vision_tower和language_model进行训练\n",
    "            \"\"\"\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 函数加载模型工具类\n",
    "def load_model_processor(modelargs: ModelArguments):\n",
    "    # 读取模型\n",
    "    model = LlavaForConditionalGeneration.from_pretrained(\n",
    "        modelargs.model_name_or_path,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        low_cpu_mem_usage=True,\n",
    "    )\n",
    "    # 读取处理器\n",
    "    processor = LlavaProcessor.from_pretrained(modelargs.model_name_or_path)\n",
    "\n",
    "    if modelargs.train_type == \"use_lora\":  # 指定使用lora训练，配置lora的相关的参数\n",
    "        logging.warning(\"Loading model to Lora\")\n",
    "\n",
    "        from peft import LoraConfig, get_peft_model\n",
    "\n",
    "        \"\"\"\n",
    "            TODO: 可以不用lora, 因为参数比较少, 再训练几次\n",
    "            引入lora的配置可能引入了一个新的变量, 会导致实验的不严谨性\n",
    "        \"\"\"\n",
    "        \n",
    "        LORA_R = 32\n",
    "        # LORA_ALPHA = 16\n",
    "        LORA_DROPOUT = 0.05\n",
    "        TARGET_MODULES = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n",
    "\n",
    "        config = LoraConfig(\n",
    "            r=LORA_R,\n",
    "            # lora_alpha=LORA_ALPHA,\n",
    "            target_modules=TARGET_MODULES,\n",
    "            lora_dropout=LORA_DROPOUT,\n",
    "            bias=\"none\",\n",
    "            task_type=\"CAUSAL_LM\",\n",
    "            modules_to_save=[\"multi_modal_projector\"],  # 显式指定训练从视觉层投影到文本层的MLP\n",
    "        )\n",
    "        model = get_peft_model(model, config)\n",
    "        # model.print_trainable_parameters()\n",
    "\n",
    "    elif modelargs.train_type == \"none\":\n",
    "        logging.warning(\"使用全量参数进行训练\")\n",
    "\n",
    "        pass\n",
    "    elif modelargs.train_type == \"freeze_vision\":\n",
    "        logging.warning(\"冻结vision_tower网络层，剩下的网络权重进行训练\")\n",
    "\n",
    "        for param in model.vision_tower.parameters():\n",
    "            param.requires_grad = False\n",
    "    elif modelargs.train_type == \"freeze_vision_and_language\":\n",
    "        logging.warning(\"llava stage1 冻结vision_tower和language_model网络层, 剩下的网络权重进行训练\")\n",
    "        \n",
    "        # 冻结 vision_tower 网络层\n",
    "        for param in model.vision_tower.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # 冻结 language_model 网络层\n",
    "        for param in model.language_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # 显示指定 multi_modal_projector 层参与梯度更新\n",
    "        for param in model.multi_modal_projector.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    print_trainable_parameters(model)   # 打印此时可训练的参数占全部参数的百分比\n",
    "    return model, processor\n",
    "\n",
    "\n",
    "def load_dataset_collator(processor, dataargs: DataArguments):\n",
    "\n",
    "    llava_dataset = LlavaDataset(\n",
    "        dataargs.data_path  # \"data/liuhaotian/LLaVA-CC3M-Pretrain-595K\"\n",
    "    )\n",
    "    data_collator = TrainLLavaModelCollator(processor, -100)\n",
    "\n",
    "    return llava_dataset, data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model_args: ModelArguments = ModelArguments(\n",
    "        model_name_or_path=\"./qwen2.5_3B_Instruct_clipvL14_model/model001\",\n",
    "        train_type=\"freeze_vision_and_language\"\n",
    "    )\n",
    "\n",
    "    data_args: DataArguments = DataArguments(\n",
    "        data_path=\"/home/lsy/shared_data/liuhaotian/LLaVA-CC3M-Pretrain-595K\"\n",
    "    )\n",
    "    \n",
    "    model, processor = load_model_processor(model_args)\n",
    "    train_dataset, data_collator = load_dataset_collator(processor, data_args)\n",
    "    \n",
    "    eval_dataset = train_dataset.train_test_split(test_size=0.1)\n",
    "\n",
    "\n",
    "    # trainer = Trainer(\n",
    "    #     model=model,\n",
    "    #     args=training_args,\n",
    "    #     train_dataset=train_dataset,\n",
    "    #     eval_dataset=None,\n",
    "    #     data_collator=data_collator,\n",
    "    # )\n",
    "\n",
    "    # trainer.train()\n",
    "    # trainer.save_state()\n",
    "    # trainer.save_model(output_dir=training_args.output_dir)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "        format=\"%(asctime)s %(levelname)s [%(name)s] %(message)s\",\n",
    "        level=logging.INFO,\n",
    "        datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    ")\n",
    "train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llava",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
