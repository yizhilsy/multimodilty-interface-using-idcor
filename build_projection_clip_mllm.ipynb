{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用带有`visual_projection`的clip模型，这样的clip模型允许输出`image_embeds`信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (  AutoModel, \n",
    "                            AutoModelForCausalLM, \n",
    "                            AutoTokenizer, \n",
    "                            AutoProcessor,\n",
    "                            CLIPVisionModelWithProjection,\n",
    "                            LlavaForConditionalGeneration,\n",
    "                            LlavaConfig\n",
    "                        )\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_model_name_or_path = (\n",
    "    \"./openai/clip-vit-large-patch14-336\"\n",
    ")\n",
    "qwen_model_name_or_path = \"./Qwen/Qwen2.5-3B-Instruct\"\n",
    "device = \"cuda:0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 加载`clip`，`qwen`， `qwen_tokenizer`\n",
    "#### 使用`CLIPVisionModelWithProjection`来加载clip模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lsy/anaconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10b6ebafa7f3474c8ad23747d419ce20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "clip_model = CLIPVisionModelWithProjection.from_pretrained(clip_model_name_or_path, device_map=device)\n",
    "llm_model = AutoModelForCausalLM.from_pretrained(\n",
    "    qwen_model_name_or_path, device_map=device\n",
    ")\n",
    "llm_tokenizer = AutoTokenizer.from_pretrained(qwen_model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([151665],\n",
       " CLIPVisionModelWithProjection(\n",
       "   (vision_model): CLIPVisionTransformer(\n",
       "     (embeddings): CLIPVisionEmbeddings(\n",
       "       (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
       "       (position_embedding): Embedding(577, 1024)\n",
       "     )\n",
       "     (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "     (encoder): CLIPEncoder(\n",
       "       (layers): ModuleList(\n",
       "         (0-23): 24 x CLIPEncoderLayer(\n",
       "           (self_attn): CLIPSdpaAttention(\n",
       "             (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "           )\n",
       "           (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "           (mlp): CLIPMLP(\n",
       "             (activation_fn): QuickGELUActivation()\n",
       "             (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "             (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "           )\n",
       "           (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "   )\n",
       "   (visual_projection): Linear(in_features=1024, out_features=768, bias=False)\n",
       " ))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_tokenizer.encode(\"<image>\"), clip_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 将clip模型和llm_model模型的config拿出来，初始化一个llava config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing a CLIP-vision config\n",
    "vision_config = clip_model.vision_model.config\n",
    "\n",
    "# Initializing a Llama config\n",
    "text_config = llm_model.config\n",
    "\n",
    "# Initializing a Llava llava-1.5-7b style configuration\n",
    "configuration = LlavaConfig(vision_config, text_config)\n",
    "\n",
    "# Initializing a model from the llava-1.5-7b style configuration\n",
    "model = LlavaForConditionalGeneration(configuration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 但是上面，只是把llava模型的形状初始化好了，模型权重都还是随机生成的，需要把两个模型的权重，复制过去"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.vision_tower.vision_model = clip_model.vision_model\n",
    "model.language_model = llm_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 将Qwen tokenizer的`pad_token_id`及`image_token_index`赋值给llava架构的config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(151643, 151665)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.pad_token_id = llm_tokenizer.pad_token_id\n",
    "model.config.image_token_index = llm_tokenizer.encode(\"<image>\")[0]\n",
    "model.config.pad_token_id, model.config.image_token_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 保存`model`，`tokenizer`，`图像模型processor`，并且需要把`model002`里面的`preprocessor_config.json`文件，放在`model001`里面"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-02-16 16:33:01,223] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"qwen2.5_3B_Instruct_clipvL14_model/clip_proj/model001\")\n",
    "llm_tokenizer.save_pretrained(\"qwen2.5_3B_Instruct_clipvL14_model/clip_proj/model001\")\n",
    "autoprocessor = AutoProcessor.from_pretrained(clip_model_name_or_path)\n",
    "autoprocessor.save_pretrained(\"qwen2.5_3B_Instruct_clipvL14_model/clip_proj/model002\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### reboot并且测试模型是否可以正常工作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25a3d0144b0c48449cdcad530b80d9a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import LlavaProcessor, LlavaForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "\n",
    "model_name_or_path = \"./qwen2.5_3B_Instruct_clipvL14_model/clip_proj/model001\"\n",
    "# model_name_or_path = \"test_model_copy/model001\"  #\n",
    "\n",
    "llava_processor = LlavaProcessor.from_pretrained(model_name_or_path)\n",
    "model = LlavaForConditionalGeneration.from_pretrained(\n",
    "    model_name_or_path, device_map=\"cuda:0\", torch_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Expanding inputs for image tokens in LLaVa should be done in processing. Please add `patch_size` and `vision_feature_select_strategy` to the model's processing config or set directly with `processor.patch_size = {{patch_size}}` and processor.vision_feature_select_strategy = {{vision_feature_select_strategy}}`. Using processors without these attributes in the config is deprecated and will throw an error in v4.50.\n",
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "Expanding inputs for image tokens in LLaVa should be done in processing. Please add `patch_size` and `vision_feature_select_strategy` to the model's processing config or set directly with `processor.patch_size = {{patch_size}}` and processor.vision_feature_select_strategy = {{vision_feature_select_strategy}}`. Using processors without these attributes in the config is deprecated and will throw an error in v4.50.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "<image>\n",
      "What are these?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "These are aa series of Chinese characters and words mixed with English words and phrases, which appear to be a jumbled mess of Chinese text and English. It doesn't form a coherent sentence or meaningful phrase. The text seems to be a random combination of terms related to \"travel\" (旅游), \"business\" (商务), \"law\" (法律), \"education\" (教育), and \"communication\" (沟通) in Chinese, mixed with some English words and phrases.\n",
      "\n",
      "It's not clear what the original message was trying to convey, as it appears to be a mix-up of different concepts and terms. Without more context, it's difficult to provide a specific interpretation or meaning.<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "prompt_text = \"<image>\\nWhat are these?\"\n",
    "\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt_text},\n",
    "]\n",
    "prompt = llava_processor.tokenizer.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "\n",
    "\n",
    "image_path = \"./data/000000039769.jpg\"\n",
    "image = Image.open(image_path)\n",
    "\n",
    "inputs = llava_processor(text=prompt, images=image, return_tensors=\"pt\")\n",
    "\n",
    "for tk in inputs.keys():\n",
    "    inputs[tk] = inputs[tk].to(model.device)\n",
    "\n",
    "generate_ids = model.generate(**inputs, max_new_tokens=200)\n",
    "gen_text = llava_processor.batch_decode(\n",
    "    generate_ids, skip_special_tokens=False, clean_up_tokenization_spaces=False\n",
    ")[0]\n",
    "\n",
    "print(gen_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask', 'pixel_values'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 25]), torch.Size([1, 25]), torch.Size([1, 3, 336, 336]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs['input_ids'].shape, inputs['attention_mask'].shape, inputs['pixel_values'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[151644,   8948,    198,   2610,    525,    264,  10950,  17847,     13,\n",
       "          151645,    198, 151644,    872,    198, 151665,    198,   3838,    525,\n",
       "            1493,     30, 151645,    198, 151644,  77091,    198]],\n",
       "        device='cuda:0'),\n",
       " tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1]], device='cuda:0'),\n",
       " tensor([[[[ 0.5435,  0.6457,  0.5581,  ...,  0.0909,  0.0033, -0.0696],\n",
       "           [ 0.5435,  0.6165,  0.5435,  ...,  0.1201,  0.0179,  0.0617],\n",
       "           [ 0.5581,  0.5581,  0.6603,  ...,  0.0909,  0.0763,  0.0617],\n",
       "           ...,\n",
       "           [ 1.8281,  1.8865,  1.8281,  ...,  1.4048,  1.4486,  1.5654],\n",
       "           [ 1.8573,  1.9011,  1.8719,  ...,  1.4778,  1.4048,  1.4924],\n",
       "           [ 1.8719,  1.9011,  1.9011,  ...,  1.4048,  1.2150,  1.4778]],\n",
       " \n",
       "          [[-1.3619, -1.2718, -1.3769,  ..., -1.4219, -1.4820, -1.5120],\n",
       "           [-1.3319, -1.2418, -1.3469,  ..., -1.4219, -1.4820, -1.4219],\n",
       "           [-1.2418, -1.2869, -1.1968,  ..., -1.4669, -1.4669, -1.4820],\n",
       "           ...,\n",
       "           [ 0.0789,  0.1239,  0.0338,  ..., -0.7166, -0.6565, -0.5665],\n",
       "           [ 0.1089,  0.1089,  0.0789,  ..., -0.6265, -0.7166, -0.6265],\n",
       "           [ 0.1239,  0.1089,  0.0789,  ..., -0.6415, -0.8816, -0.5515]],\n",
       " \n",
       "          [[-0.5559, -0.3853, -0.4137,  ..., -0.8688, -0.8545, -0.8688],\n",
       "           [-0.4564, -0.4422, -0.4848,  ..., -0.8119, -0.8830, -0.7834],\n",
       "           [-0.5275, -0.4422, -0.3995,  ..., -0.8688, -0.8261, -0.8403],\n",
       "           ...,\n",
       "           [ 1.6055,  1.5771,  1.5629,  ...,  0.8519,  0.7666,  0.8092],\n",
       "           [ 1.6055,  1.6624,  1.6624,  ...,  0.7808,  0.8661,  0.6670],\n",
       "           [ 1.6482,  1.6482,  1.6624,  ...,  0.8377,  0.8945,  0.8234]]]],\n",
       "        device='cuda:0'))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs['input_ids'], inputs['attention_mask'], inputs['pixel_values']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llava",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
